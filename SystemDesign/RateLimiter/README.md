## What is a Rate Limiter?
A rate limiter, at a high-level, limits the number of events an entity (user, device, IP, etc.) can perform in a particular time window. For example:

* A user can send only one message per second.
* A user is allowed only three failed credit card transactions per day.
* A single IP can only create twenty accounts per day.

In general, a rate limiter caps how many requests a sender can issue in a specific time window. It then blocks requests once the cap is reached.

## Need of API rate limiting
Rate Limiting helps to protect services against abusive behaviors targeting the application layer like Denial-of-service (DOS) attacks, brute-force password attempts, brute-force credit card transactions, etc. These attacks are usually a barrage of HTTP/S requests which may look like they are coming from real users, but are typically generated by machines (or bots). As a result, these attacks are often harder to detect and can more easily bring down a service, application, or an API.

Rate limiting is also used to prevent revenue loss, to reduce infrastructure costs, to stop spam and online harassment. Following is a list of scenarios that can benefit from Rate limiting by making a service (or API) more reliable:

* Misbehaving clients/scripts: Example scenario could be when a user is sending a lot of lower-priority requests, and we want to make sure that it doesn’t affect the high-priority traffic. For example, users sending a high volume of requests for analytics data should not be allowed to hamper critical transactions for other users.
* Security: By limiting the number of the second-factor attempts (in 2-factor auth) that the users are allowed to perform, for example, the number of times they’re allowed to try with a wrong password.
* To keep costs and resource usage under control
* Revenue: Certain services might want to limit operations based on the tier of their customer’s service, and thus create a revenue model based on rate limiting. There could be default limits for all the APIs a service offers. To go beyond that, the user has to buy higher limits
* To eliminate spikiness in traffic: So that a service stays up for everyone else.

## How to do Rate Limiting?
Rate Limiting is a process that is used to define the rate and speed at which consumers can access APIs. Throttling can be defined at the application level and/or API level. When a throttle limit is crossed, the server returns “429” as HTTP status to the user with message content as “Too many requests”.

## What are different types of throttling?

* Hard Throttling: The number of API requests cannot exceed the throttle limit.
* Soft Throttling: In this type, we can set the API request limit to exceed a certain percentage. For example, if we have rate-limit of 100 messages a minute and 10% exceed limit. Our rate limiter will allow up to 110 messages per minute.
* Elastic or Dynamic Throttling: Under Elastic throttling, the number of requests can go beyond the threshold if the system has some resources available. For example, if a user is allowed only 100 messages a minute, we can let the user send more than 100 messages a minute if there are free resources available in the system.

# What are different types of algorithms used for Rate Limiting?
Following are the two types of algorithms used for Rate Limiting:

* **Fixed Window Algorithm:** In this algorithm, the time window is considered from the start of the time-unit to the end of the time-unit. For example, a period would be considered as 0-60 seconds for a minute irrespective of the time frame at which the API request has been made. In the diagram below, there are two messages between 0-1 second and three messages between 1-2 second. If we’ve a rate limiting of two messages a second, this algorithm will reject m5.

![image](https://user-images.githubusercontent.com/6800366/38092641-7087aad2-3386-11e8-99fa-9fbb40dc3686.png)

* **Rolling Window Algorithm:** In this algorithm, the time window is considered from the fraction of the time at which the request is made plus the time window length. For example, if there are two messages sent at 300th millisecond and 400th millisecond of a second, we’ll count them as two messages from 300th millisecond of that second up to the 300th millisecond of next second. In the above diagram, keeping two messages a second, we’ll throttle ‘m3’ and ‘m4’.

## High level design for Rate Limiter
Once a new request arrives, Web Server first asks the Rate Limiter to decide if it will be served or throttled. If the request is not throttled, then it’ll be passed to the API servers.

![image](https://user-images.githubusercontent.com/6800366/38092757-c15782e8-3386-11e8-8a01-5e1c2fdea4b3.png)

## System Design and Algorithm

**limit the number of requests per user**
for each unique user, we would keep a count representing how many requests the user has made and a timestamp when we started counting the requests. We can keep it in a hashtable, where the ‘key’ would be the ‘UserID’ and ‘value’ would be a structure containing an integer for the ‘Count’ and an integer for the Epoch time:

![image](https://user-images.githubusercontent.com/6800366/38092826-f2bab594-3386-11e8-892a-c52d9c3c85ac.png)

Assume our rate limiter is allowing three requests per minute per user, so whenever a new request comes in, our rate limiter will perform following steps:

* If the ‘UserID’ is not present in the hash-table, insert it and set the ‘Count’ to 1 and ‘StarteTime’ to the current time (normalized to a minute) , and allow the request.

* Otherwise, find the record of the ‘UserID’ and if ‘CurrentTime – StartTime >= 1 min’, set the ‘StartTime’ to the current time and ‘Count’ to 1, and allow the request.

* If ‘CurrentTime - StartTime <= 1 min’ and
    * If ‘Count < 3’, increment the Count and allow the request.
    * If ‘Count >= 3’, reject the request.

## What are the some problems with this algorithm?

* This is a Fixed Window algorithm, as we’re resetting the ‘StartTime’ at the end of every minute, which means it can potentially allow twice the number of requests per minute. Imagine if Kristie sends three requests at the last second of a minute, then she can immediately send three more requests at the very first second of the next minute, resulting in 6 requests in the span of two seconds. The solution to this problem would be a sliding window algorithm.

* Atomicity: In a distributed environment, the “read-and-then-write” behavior can create a race condition. Imagine if user’s current ‘Count’ is “2” and that they issue two more requests. If two separate processes served each of these requests and concurrently read the Count before either of them updated it, each process would think the user can have one more request and that he/she had not hit the rate limit.

![image](https://user-images.githubusercontent.com/6800366/38097822-070f72c0-3394-11e8-9cf2-7d21b3893dea.png)

If we are using Redis to store our key-value, one solution to resolve the atomicity problem is to use Redis lock for the duration of the read-update operation. This, however, would come at the expense of slowing down concurrent requests from the same user and introducing another layer of complexity. We can use Memcached, but it would have comparable complications.

If we are using a simple hash-table, we can have a custom implementation for ‘locking’ each record to solve our atomicity problems.

## Sliding Window algorithm
We can maintain a sliding window if we can keep track of each request per user. We can store the timestamp of each request in a Redis Sorted Set in our ‘value’ field of hash-table. 

For Example: UserId: {Sorted Set<UnixTime>}
             Syril: {1499818000, 1499818500, 1499818860}


Assuming our rate limiter is allowing three requests per minute per user, so whenever a new request comes in the Rate Limiter will perform following steps:

* Remove all the timestamps from the Sorted Set that are older than “CurrentTime - 1 minute”.
* Count the total number of elements in the sorted set. Reject the request if this count is greater than our throttling limit of 3.
* Insert the current time in the sorted set, and accept the request.

![rate limiter](https://user-images.githubusercontent.com/6800366/38122432-937fe092-33f2-11e8-899f-d775e3c1e4c6.PNG)


